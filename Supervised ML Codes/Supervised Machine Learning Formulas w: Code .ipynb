{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Model: Multiple Linear Regression\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b $$\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b $$ \n",
    "\n",
    "## Cost Function: Mean Squared Error (MSE)\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 $$\n",
    "\n",
    "\n",
    "## Batch Gradient Descent \n",
    "$$\\begin{align*}\n",
    "&\\text{repeat until convergence:} \\; \\lbrace \\\\\n",
    "&  \\; \\; \\;w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\; & \\text{for j := 0..n-1} \\\\ \n",
    "&  \\; \\; \\;  \\; \\;b = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\\\\n",
    "&\\rbrace \\text{simultaneous updates}\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_x(X, w, b):\n",
    "    \"\"\"\n",
    "    Calculates the predicted values for linear regression.\n",
    "\n",
    "    Args:\n",
    "        X (ndarray): Shape (m, n) Input features, where m is the number of training examples and n is the number of features.\n",
    "        w (ndarray): Shape (n,) Model weights, one for each feature.\n",
    "        b (scalar): Bias term, a single value added to the weighted sum.\n",
    "\n",
    "    Returns:\n",
    "        f_x (ndarray): Shape (m,) Predicted values for each training example.\n",
    "    \"\"\"\n",
    "    # Number of Training Examples\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # An Array Which Will Store the Predictions of y\n",
    "    # One for Each Training Example\n",
    "    f_x = np.zeros(m)\n",
    "    \n",
    "    for i in range(m):\n",
    "        f_x[i] = np.dot(w, X[i]) + b\n",
    "    \n",
    "    return f_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Usage: Multiple Linear Regression \"\"\"\n",
    "\n",
    "def func_x_vec(X, w, b):\n",
    "    \"\"\"\n",
    "    Vectorized function to calculate predicted values for linear regression.\n",
    "\n",
    "    Args:\n",
    "        X (ndarray): Shape (m, n) Input features, where m is the number of training examples and n is the number of features.\n",
    "        w (ndarray): Shape (n,) Model weights, one for each feature.\n",
    "        b (scalar): Bias term, a single value added to the weighted sum of features.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Predicted values for each training example, shape (m,).\n",
    "    \"\"\"\n",
    "    # Vectorized Calculation of Predictions\n",
    "    return np.dot(X, w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the cost function for linear regression (J(w,b)).\n",
    "    \n",
    "    Args:\n",
    "        x (ndarray): Shape (m,) Input to the Model \n",
    "        y (ndarray): Shape (m,) Label \n",
    "        w, b (scalar): Parameters of the Model\n",
    "    \n",
    "    Returns\n",
    "        J (float): The cost of using w,b as the parameters for linear regression\n",
    "                   to fit the data points in x and y\n",
    "    \"\"\"\n",
    "    # Number of Training Examples\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Cost\n",
    "    J = 0\n",
    "    \n",
    "    # For Each Row (Observation)\n",
    "    for i in range(m):\n",
    "        \n",
    "        # Compute the Predicted Value\n",
    "        f_wb_i = np.dot(w,X[i])+b\n",
    "        \n",
    "        # Compute the Error (Predicted Value - Target Value)\n",
    "        error_i = (f_wb_i - y[i])\n",
    "        \n",
    "        # (Add Squared Error to the Cost Function)\n",
    "        J += error_i**2\n",
    "    \n",
    "    # Divide the Sum of Squared Errors by (2 * m)\n",
    "    J /= (2*m)\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Usage: Multiple Linear Regression \"\"\"\n",
    "\n",
    "def compute_cost_vec(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the cost function for linear regression (J(w,b)). \n",
    "    Vectorized.\n",
    "    \n",
    "    Args:\n",
    "        x (ndarray): Shape (m,) Input to the Model \n",
    "        y (ndarray): Shape (m,) Label \n",
    "        w, b (scalar): Parameters of the Model\n",
    "    \n",
    "    Returns\n",
    "        J (float): The cost of using w,b as the parameters for linear regression\n",
    "                   to fit the data points in x and y\n",
    "    \"\"\"\n",
    "    # Number of Training Examples\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # An Array Which Will Store the Predictions of y\n",
    "    f_x = np.dot(X, w) + b\n",
    "    \n",
    "    # Error Array (Predicted Value - Target Value)\n",
    "    error_arr = f_x - y\n",
    "    \n",
    "    # Vectorized Cost Calculation\n",
    "    J = np.sum(error_arr**2) / (2*m)\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the Gradients for Linear Regression.\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m Examples with n Features\n",
    "      y (ndarray (m,)) : Target Values\n",
    "      w (ndarray (n,)) : Model Parameters  \n",
    "      b (scalar)       : Model Parameter\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # dJ/dw has a Shape of (1xn)\n",
    "    dj_dw = np.zeros(n)\n",
    "    dj_db = 0\n",
    "    \n",
    "    # For Each Training Example m\n",
    "    for i in range(m):\n",
    "        # Calculate the Error\n",
    "        error_i = (np.dot(w, X[i]) + b) - y[i]\n",
    "    \n",
    "        # For Each Feature n\n",
    "        for j in range(n):\n",
    "            # Update the Gradient of the Corresponding Weight w[j] \n",
    "            # by Adding the Product of the Error and the Current Feature Value\n",
    "            dj_dw[j] += error_i * X[i, j]\n",
    "        \n",
    "        # Update the dJ/db by Adding the Corresponding Row's Error\n",
    "        dj_db += error_i\n",
    "    \n",
    "    dj_dw /= m\n",
    "    dj_db /= m\n",
    "    \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Usage: Multiple Linear Regression \"\"\"\n",
    "\n",
    "def compute_gradients_vec(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the Gradients for Linear Regression.\n",
    "    Vectorized.\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m Examples with n Features\n",
    "      y (ndarray (m,)) : Target Values\n",
    "      w (ndarray (n,)) : Model Parameters  \n",
    "      b (scalar)       : Model Parameter\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # The Predictions of y\n",
    "    f_x = np.dot(X, w) + b\n",
    "    \n",
    "    # Error\n",
    "    error = f_x - y\n",
    "    # For each feature across all training examples, multiply its value by how much \n",
    "    # off our prediction was (the error), sum this up for each feature, and then average it.\n",
    "    dj_dw = np.dot(X.T, error) / m\n",
    "    dj_db = np.sum(error) / m\n",
    "    \n",
    "    \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_init, b_init, alpha, num_iters, compute_cost, compute_gradients):\n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn w and b. Updates w and b by taking \n",
    "    num_iters gradient steps with learning rate alpha.\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))   : Data, m examples with n features\n",
    "      y (ndarray (m,))    : target values\n",
    "      w_in (ndarray (n,)) : initial model parameters  \n",
    "      b_in (scalar)       : initial model parameter\n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,)) : Updated values of parameters \n",
    "      b (scalar)       : Updated value of parameter \n",
    "      J_hist (List): History of cost values\n",
    "      p_hist (list): History of parameters [w,b] \n",
    "    \"\"\"\n",
    "    \n",
    "    w = w_init\n",
    "    b = b_init\n",
    "    \n",
    "    # Store Cost (J) and Parameters (w,b)\n",
    "    J_hist = []\n",
    "    p_hist = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calculate the Partial Derivatives wrt. the Parameters\n",
    "        dj_dw, dj_db = compute_gradients(X, y, w, b)\n",
    "        \n",
    "        # Update the Parameters\n",
    "        w = w - (alpha * dj_dw)\n",
    "        b = b - (alpha * dj_db)\n",
    "        \n",
    "        # Save Cost J at Each Iteration Less than 100000 to Prevent Resource Exhaustion\n",
    "        if i<100000:      \n",
    "            cost = compute_cost(X, y, w, b)\n",
    "            J_hist.append(cost)\n",
    "            p_hist.append([w,b])\n",
    "        \n",
    "        # Print the Cost\n",
    "        if i % math.ceil(num_iters/10) == 0:\n",
    "            print(f\"Iteration {i:4}: Cost {J_hist[-1]:0.2e} w:{w} b:{b:0.5e}\")\n",
    "        \n",
    "    return w, b, J_hist, p_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: L2 Regularized (Ridge) Linear Regression\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b $$\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b $$ \n",
    "\n",
    "## Cost Function: Regularized Mean Squared Error (MSE)\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2  + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2 $$\n",
    "\n",
    "## Regularized Batch Gradient Descent\n",
    "$$\\begin{align*}\n",
    "&\\text{repeat until convergence:} \\; \\lbrace \\\\\n",
    "&  \\; \\; \\;w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\; & \\text{for j := 0..n-1} \\\\ \n",
    "&  \\; \\; \\;  \\; \\;b = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\\\\n",
    "&\\rbrace \\text{simultaneous updates}\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} = \\left( \\frac{1}{m}  \\sum_{i=0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) x_j^{(i)} \\right) + \\frac{\\lambda}{m} w_j  \\quad\\, \\mbox{for $j=0...(n-1)$}$$\n",
    "$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b} = \\frac{1}{m}  \\sum_{i=0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_linear_reg(X, y, w, b, lambda_ = 1):\n",
    "    \"\"\"\n",
    "    Computes the L2-Regularized MSE Cost over All Examples.\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "    Returns:\n",
    "      total_cost (scalar):  cost \n",
    "    \"\"\"\n",
    "\n",
    "    m  = X.shape[0]\n",
    "    n  = len(w)\n",
    "    cost = 0.\n",
    "    for i in range(m):\n",
    "        f_wb_i = np.dot(X[i], w) + b                                   \n",
    "        cost = cost + (f_wb_i - y[i])**2                              \n",
    "    cost = cost / (2 * m)                                             \n",
    " \n",
    "    reg_cost = 0\n",
    "    for j in range(n):\n",
    "        reg_cost += (w[j]**2)                                          \n",
    "    reg_cost = reg_cost * (lambda_/(2*m))                    \n",
    "    \n",
    "    total_cost = cost + reg_cost                                    \n",
    "    return total_cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Usage: Multiple Linear Regression \"\"\"\n",
    "\n",
    "def compute_cost_linear_reg_vec(X, y, w, b, lambda_ = 1):\n",
    "    \"\"\"\n",
    "    Computes the L2-Regularized MSE Cost over All Examples.\n",
    "    Vectorized\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "    Returns:\n",
    "      total_cost (scalar):  cost \n",
    "    \"\"\"\n",
    "    # Number of Training Examples\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # An Array Which Will Store the Predictions of y\n",
    "    f_x = np.dot(X, w) + b\n",
    "    \n",
    "    # Error Array (Predicted Value - Target Value)\n",
    "    error_arr = f_x - y\n",
    "    \n",
    "    # Vectorized Cost Calculation for Linear Regression\n",
    "    J = np.sum(error_arr**2) / (2*m)\n",
    "    \n",
    "    # Regularization Term (Excluding the Bias Term)\n",
    "    reg_cost = np.sum(w**2) * (lambda_ / (2*m))\n",
    "    \n",
    "    # Total Cost with Regularization\n",
    "    total_cost = J + reg_cost\n",
    "    \n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_linear_reg(X, y, w, b, lambda_): \n",
    "    \"\"\"\n",
    "    Computes the Gradients for L2-Regularized Linear Regression.\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m,n = X.shape           \n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):                             \n",
    "        err = (np.dot(X[i], w) + b) - y[i]                 \n",
    "        for j in range(n):                         \n",
    "            dj_dw[j] = dj_dw[j] + err * X[i, j]               \n",
    "        dj_db = dj_db + err                        \n",
    "    dj_dw = dj_dw / m                                \n",
    "    dj_db = dj_db / m   \n",
    "    \n",
    "    for j in range(n):\n",
    "        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]\n",
    "\n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Usage: Multiple Linear Regression \"\"\"\n",
    "\n",
    "def compute_gradient_linear_reg_vec(X, y, w, b, lambda_):\n",
    "    \"\"\"\n",
    "    Computes the Gradients for L2-Regularized Linear Regression.\n",
    "    Vectorized.\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m, n = X.shape  \n",
    "\n",
    "    predictions = np.dot(X, w) + b\n",
    "\n",
    "    error = predictions - y\n",
    "\n",
    "    dj_dw = (np.dot(X.T, error) + lambda_ * w) / m\n",
    "    dj_db = np.sum(error) / m\n",
    "\n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: Logistic Regression\n",
    "#### g = Sigmoid Function (Logistic Function)\n",
    "$$\n",
    "\\begin{align}\n",
    "  f_{\\mathbf{w},b}(\\mathbf{x^{(i)}}) &= g(z^{(i)}) \\\\\n",
    "  z^{(i)} &= \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+ b \\\\\n",
    "  g(z^{(i)}) &= \\frac{1}{1+e^{-z^{(i)}}}\n",
    "\\end{align}\n",
    "$$\n",
    "  \n",
    "$$f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = P(y=1 \\mid \\mathbf{x}; \\mathbf{w}, b)$$\n",
    "If $f_{\\mathbf{w},b}(x) >= 0.5$, Predict $y=1$\n",
    "\n",
    "If $f_{\\mathbf{w},b}(x) < 0.5$, Predict $y=0$\n",
    "\n",
    "#### Decision Boundary: $z = \\mathbf{w} \\cdot \\mathbf{x} + b = 0$\n",
    "\n",
    "  If $\\mathbf{w} \\cdot \\mathbf{x} + b >= 0$, the Model Predicts $y=1$\n",
    "  \n",
    "  If $\\mathbf{w} \\cdot \\mathbf{x} + b < 0$, the Model Predicts $y=0$\n",
    " \n",
    "## Cost Function:  Log-Loss (Binary Cross-Entropy Loss)\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{m}  \\sum_{i=0}^{m-1} \\left[ -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\right]\n",
    "$$\n",
    "\n",
    "## Batch Gradient Descent\n",
    "$$\\begin{align*}\n",
    "&\\text{repeat until convergence:} \\; \\lbrace \\\\\n",
    "&  \\; \\; \\;w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\; & \\text{for j := 0..n-1} \\\\ \n",
    "&  \\; \\; \\;  \\; \\;b = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\\\\n",
    "&\\rbrace \\text{simultaneous updates}\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the Sigmoid of z.\n",
    "\n",
    "    Args:\n",
    "        z (ndarray): A scalar, numpy array of any size.\n",
    "\n",
    "    Returns:\n",
    "        g (ndarray): sigmoid(z), with the same shape as z\n",
    "    \"\"\"\n",
    "    g = 1/(1+np.exp(-z))\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes Cost (Log Loss/ Binary Cross-Entropy Loss).\n",
    "\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[0]\n",
    "    cost = 0.0\n",
    "    for i in range(m):\n",
    "        z_i = np.dot(X[i],w) + b\n",
    "        f_wb_i = sigmoid(z_i)\n",
    "        cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)\n",
    "             \n",
    "    cost = cost / m\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Usage: Multiple Features \"\"\"\n",
    "\n",
    "def compute_cost_vec(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes Cost (Log Loss/ Binary Cross-Entropy Loss).\n",
    "    Vectorized.\n",
    "\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar): model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[0]\n",
    "\n",
    "    z = np.dot(X, w) + b\n",
    "    f_wb = sigmoid(z)\n",
    "\n",
    "    cost = -np.sum(y * np.log(f_wb) + (1 - y) * np.log(1 - f_wb)) / m\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_logistic_reg(X, y, w, b, lambda_): \n",
    "    \"\"\"\n",
    "    Computes the Gradients for Logistic Regression.\n",
    " \n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "    Returns\n",
    "      dj_dw (ndarray Shape (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar)            : The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m,n = X.shape\n",
    "    dj_dw = np.zeros((n,))                            \n",
    "    dj_db = 0.0                                       \n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = sigmoid(np.dot(X[i],w) + b)         \n",
    "        err_i  = f_wb_i  - y[i]                      \n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      \n",
    "        dj_db = dj_db + err_i\n",
    "    dj_dw = dj_dw/m                                   \n",
    "    dj_db = dj_db/m                                   \n",
    "\n",
    "    return dj_db, dj_dw  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Usage: Multiple Features \"\"\"\n",
    "\n",
    "def compute_gradient_logistic_reg_vec(X, y, w, b, lambda_):\n",
    "    \"\"\"\n",
    "    Computes the Gradients for L2-Regularized Logistic Regression.\n",
    "    Vectorized.\n",
    " \n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "    Returns\n",
    "      dj_dw (ndarray Shape (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar)            : The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m, n = X.shape \n",
    "\n",
    "    z = np.dot(X, w) + b\n",
    "    f_wb = sigmoid(z)\n",
    "\n",
    "    error = f_wb - y\n",
    "    \n",
    "    dj_dw = np.dot(X.T, error) / m\n",
    "    dj_db = np.sum(error) / m\n",
    "\n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: L2 Regularized (Ridge) Logistic Regression\n",
    "#### g = Sigmoid Function (Logistic Function)\n",
    "$$\n",
    "\\begin{align}\n",
    "  f_{\\mathbf{w},b}(\\mathbf{x^{(i)}}) &= g(z^{(i)}) \\\\\n",
    "  z^{(i)} &= \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+ b \\\\\n",
    "  g(z^{(i)}) &= \\frac{1}{1+e^{-z^{(i)}}}\n",
    "\\end{align}\n",
    "$$\n",
    "  \n",
    "$$f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = P(y=1 \\mid \\mathbf{x}; \\mathbf{w}, b)$$\n",
    "If $f_{\\mathbf{w},b}(x) >= 0.5$, Predict $y=1$\n",
    "\n",
    "If $f_{\\mathbf{w},b}(x) < 0.5$, Predict $y=0$\n",
    "\n",
    "#### Decision Boundary: $z = \\mathbf{w} \\cdot \\mathbf{x} + b = 0$\n",
    "\n",
    "  If $\\mathbf{w} \\cdot \\mathbf{x} + b >= 0$, the Model Predicts $y=1$\n",
    "  \n",
    "  If $\\mathbf{w} \\cdot \\mathbf{x} + b < 0$, the Model Predicts $y=0$\n",
    "\n",
    "## Cost Function:  Regularized Log-Loss (Regularized Binary Cross-Entropy Loss)\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{m}  \\sum_{i=0}^{m-1} \\left[ -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$$\n",
    "\n",
    "## Regularized Batch Gradient Descent \n",
    "$$\\begin{align*}\n",
    "&\\text{repeat until convergence:} \\; \\lbrace \\\\\n",
    "&  \\; \\; \\;w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\; & \\text{for j := 0..n-1} \\\\ \n",
    "&  \\; \\; \\;  \\; \\;b = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\\\\n",
    "&\\rbrace \\text{simultaneous updates}\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} = \\left( \\frac{1}{m}  \\sum_{i=0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) x_j^{(i)} \\right) + \\frac{\\lambda}{m} w_j  \\quad\\, \\mbox{for $j=0...(n-1)$}$$\n",
    "$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b} = \\frac{1}{m}  \\sum_{i=0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_logistic_reg(X, y, w, b, lambda_ = 1):\n",
    "    \"\"\"\n",
    "     Computes the L2-Regularized Log-Loss Cost over All Examples.\n",
    "     \n",
    "    Args:\n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "    Returns:\n",
    "      total_cost (scalar):  cost \n",
    "    \"\"\"\n",
    "\n",
    "    m,n  = X.shape\n",
    "    cost = 0.\n",
    "    for i in range(m):\n",
    "        z_i = np.dot(X[i], w) + b                                      \n",
    "        f_wb_i = 1 / (1 + np.exp(-z_i))                                       \n",
    "        cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)     \n",
    "             \n",
    "    cost = cost/m                                                      \n",
    "\n",
    "    reg_cost = 0\n",
    "    for j in range(n):\n",
    "        reg_cost += (w[j]**2)                                          \n",
    "    reg_cost = (lambda_/(2*m)) * reg_cost                              \n",
    "    \n",
    "    total_cost = cost + reg_cost                                      \n",
    "    return total_cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Usage: Multiple Features \"\"\"\n",
    "\n",
    "def compute_cost_logistic_reg_vec(X, y, w, b, lambda_ = 1):\n",
    "    \"\"\"\n",
    "     Computes the L2-Regularized Log-Loss Cost over All Examples.\n",
    "     Vectorized.\n",
    "     \n",
    "    Args:\n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "    Returns:\n",
    "      total_cost (scalar):  cost \n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "\n",
    "    z = np.dot(X, w) + b\n",
    "    f_wb = 1 / (1 + np.exp(-z))\n",
    "\n",
    "    cost = -np.sum(y * np.log(f_wb) + (1 - y) * np.log(1 - f_wb)) / m\n",
    "\n",
    "    reg_cost = np.sum(w**2) * (lambda_ / (2 * m))\n",
    "\n",
    "    total_cost = cost + reg_cost\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_logistic_reg(X, y, w, b, lambda_): \n",
    "    \"\"\"\n",
    "    Computes the Gradients for L2-Regularized Logistic Regression.\n",
    " \n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "    Returns\n",
    "      dj_dw (ndarray Shape (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar)            : The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m,n = X.shape\n",
    "    dj_dw = np.zeros((n,))                            \n",
    "    dj_db = 0.0                                       \n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = sigmoid(np.dot(X[i],w) + b)         \n",
    "        err_i  = f_wb_i  - y[i]                      \n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      \n",
    "        dj_db = dj_db + err_i\n",
    "    dj_dw = dj_dw/m                                   \n",
    "    dj_db = dj_db/m                                   \n",
    "\n",
    "    for j in range(n):\n",
    "        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]\n",
    "\n",
    "    return dj_db, dj_dw  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Usage: Multiple Features \"\"\"\n",
    "\n",
    "def compute_gradient_logistic_reg_vec(X, y, w, b, lambda_):\n",
    "    \"\"\"\n",
    "    Computes the Gradients for L2-Regularized Logistic Regression.\n",
    "    Vectorized.\n",
    " \n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "    Returns\n",
    "      dj_dw (ndarray Shape (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar)            : The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m, n = X.shape \n",
    "\n",
    "    z = np.dot(X, w) + b\n",
    "    f_wb = sigmoid(z)\n",
    "\n",
    "    error = f_wb - y\n",
    "    \n",
    "    dj_dw = (np.dot(X.T, error) + lambda_ * w) / m\n",
    "    dj_db = np.sum(error) / m\n",
    "\n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z-Score Normalization ## \n",
    "\n",
    "$$x^{(i)}_j = \\dfrac{x^{(i)}_j - \\mu_j}{\\sigma_j}$$ \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} x^{(i)}_j \\\\\n",
    "\\sigma^2_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} (x^{(i)}_j - \\mu_j)^2  \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_normalization(X):\n",
    "    \"\"\"\n",
    "    Computes  X, Z-Score Normalized by column.\n",
    "    After Z-Score Normalization, all features will have a mean of 0 and a standard deviation of 1.\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))     : input data, m examples, n features\n",
    "      \n",
    "    Returns:\n",
    "      X_norm (ndarray (m,n)): input normalized by column\n",
    "      mu (ndarray (n,))     : mean of each feature\n",
    "      sigma (ndarray (n,))  : standard deviation of each feature\n",
    "    \"\"\"\n",
    "    # Find the Mean of Each Column/Feature\n",
    "    # mu Will Have a Shape of (n,)\n",
    "    mu = np.mean(X, axis=0)                 \n",
    "    \n",
    "    # Find the Standard Deviation of Eeach Column/Feature\n",
    "    # sigma Will Have a Shape of (n,)\n",
    "    sigma  = np.std(X, axis=0) \n",
    "    \n",
    "    # Element-Wise,Subtract mu for That Column from Each Example, then Divide by std for That Column\n",
    "    X_norm = (X - mu) / sigma      \n",
    "\n",
    "    return X_norm, mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: K-Means Algorithm\n",
    "The K-means algorithm is a method to automatically cluster similar\n",
    "data points together. \n",
    "\n",
    "* You are given a training set $\\{x^{(1)}, ..., x^{(m)}\\}$, and you want to group the data into a few cohesive “clusters”. \n",
    "\n",
    "\n",
    "* K-means is an iterative procedure that\n",
    "     * Starts by guessing the initial centroids, and then \n",
    "     * Refines this guess by \n",
    "         * Repeatedly assigning examples to their closest centroids, and then \n",
    "         * Recomputing the centroids based on the assignments.\n",
    "         \n",
    "\n",
    "* In pseudocode, the K-means algorithm is as follows:\n",
    "\n",
    "    ``` python\n",
    "    # Initialize centroids\n",
    "    # K is the number of clusters\n",
    "    centroids = kMeans_init_centroids(X, K)\n",
    "    \n",
    "    for iter in range(iterations):\n",
    "        # Cluster assignment step: \n",
    "        # Assign each data point to the closest centroid. \n",
    "        # idx[i] corresponds to the index of the centroid \n",
    "        # assigned to example i\n",
    "        idx = find_closest_centroids(X, centroids)\n",
    "\n",
    "        # Move centroid step: \n",
    "        # Compute means based on centroid assignments\n",
    "        centroids = compute_centroids(X, idx, K)\n",
    "    ```\n",
    "    \n",
    "    \n",
    "* The inner-loop of the algorithm repeatedly carries out two steps: \n",
    "    1. Assigning each training example $x^{(i)}$ to its closest centroid, and\n",
    "    2. Recomputing the mean of each centroid using the points assigned to it. \n",
    "    \n",
    "    \n",
    "* The $K$-means algorithm will always converge to some final set of means for the centroids. \n",
    "\n",
    "* However, the converged solution may not always be ideal and depends on the initial setting of the centroids.\n",
    "    * Therefore, in practice the K-means algorithm is usually run a few times with different random initializations. \n",
    "    * One way to choose between these different solutions from different random initializations is to choose the one with the lowest cost function value (distortion)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding Closest Centroids\n",
    "\n",
    "In the “cluster assignment” phase of the K-means algorithm, the\n",
    "algorithm assigns every training example $x^{(i)}$ to its closest\n",
    "centroid, given the current positions of centroids. \n",
    "\n",
    "* This function takes the data matrix `X` and the locations of all\n",
    "centroids inside `centroids` \n",
    "* It should output a one-dimensional array `idx` (which has the same number of elements as `X`) that holds the index  of the closest centroid (a value in $\\{0,...,K-1\\}$, where $K$ is total number of centroids) to every training example . *(Note: The index range 0 to K-1 varies slightly from what is shown in the lectures (i.e. 1 to K) because Python list indices start at 0 instead of 1)*\n",
    "* Specifically, for every example $x^{(i)}$ we set\n",
    "$$c^{(i)} := j \\quad \\mathrm{that \\; minimizes} \\quad ||x^{(i)} - \\mu_j||^2,$$\n",
    "where \n",
    " * $c^{(i)}$ is the index of the centroid that is closest to $x^{(i)}$ (corresponds to `idx[i]` in the starter code), and \n",
    " * $\\mu_j$ is the position (value) of the $j$’th centroid. (stored in `centroids` in the starter code)\n",
    " * $||x^{(i)} - \\mu_j||$ is the L2-norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing Centroid Means\n",
    "\n",
    "Given assignments of every point to a centroid, the second phase of the\n",
    "algorithm recomputes, for each centroid, the mean of the points that\n",
    "were assigned to it.\n",
    "\n",
    "* Specifically, for every centroid $\\mu_k$ we set\n",
    "$$\\mu_k = \\frac{1}{|C_k|} \\sum_{i \\in C_k} x^{(i)}$$ \n",
    "\n",
    "    where \n",
    "    * $C_k$ is the set of examples that are assigned to centroid $k$\n",
    "    * $|C_k|$ is the number of examples in the set $C_k$\n",
    "\n",
    "\n",
    "* Concretely, if two examples say $x^{(3)}$ and $x^{(5)}$ are assigned to centroid $k=2$,\n",
    "then you should update $\\mu_2 = \\frac{1}{2}(x^{(3)}+x^{(5)})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_centroids(X, centroids):\n",
    "    \"\"\"\n",
    "    Computes the Centroid Memberships for Every Example.\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray): (m, n) Input values      \n",
    "        centroids (ndarray): (K, n) centroids\n",
    "    \n",
    "    Returns:\n",
    "        idx (array_like): (m,) closest centroids\n",
    "    \n",
    "    \"\"\"\n",
    "    # Set K\n",
    "    K = centroids.shape[0]\n",
    "    idx = np.zeros(X.shape[0], dtype=int)\n",
    "    \n",
    "    # Number of Training Examples\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    for i in range(m):\n",
    "        # An Array that Stores the Distance between the Example i and Each Centroid \n",
    "        distance_i = np.zeros(K)\n",
    "        \n",
    "        # Loop the Example i, K (Number of Centroids) Times and Find the L2-Norm (Squared)\n",
    "        for j in range(K):\n",
    "            distance_i[j] = np.sum(np.square(X[i] - centroids[j]))\n",
    "        \n",
    "        # Closest Centroid for the Example i is the Centroid that Gives the Lowest L2-Norm (Squared)\n",
    "        idx[i] = np.argmin(distance_i)\n",
    "            \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centroids(X, idx, K):\n",
    "    \"\"\"\n",
    "    Returns the new centroids by computing the means of the \n",
    "    data points assigned to each centroid.\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):   (m, n) Data points\n",
    "        idx (ndarray): (m,) Array containing index of closest centroid for each \n",
    "                       example in X. Concretely, idx[i] contains the index of \n",
    "                       the centroid closest to example i\n",
    "        K (int):       number of centroids\n",
    "    \n",
    "    Returns:\n",
    "        centroids (ndarray): (K, n) New centroids computed\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    centroids = np.zeros((K, n))\n",
    "    \n",
    "    \n",
    "    # Loop through Each Centroid k\n",
    "    for k in range(K):\n",
    "        # Each Centroid has n Features \n",
    "        updated_centroid_k = np.zeros(n)\n",
    "        \n",
    "        # Number of Examples in Centroid k\n",
    "        num_exampl_k = np.sum(idx == k)\n",
    "        \n",
    "        # Loop through the Data Set, if an Example belongs to Centroid k, Add it to the updated_centroid_k\n",
    "        for i in range(m):\n",
    "            if idx[i] == k:\n",
    "                updated_centroid_k += X[i]\n",
    "        \n",
    "        # Formula\n",
    "        updated_centroid_k = updated_centroid_k / num_exampl_k\n",
    "        \n",
    "        # Update the Centroid\n",
    "        centroids[k] = updated_centroid_k\n",
    "            \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kMeans(X, initial_centroids, max_iters=10):\n",
    "    \"\"\"\n",
    "    Runs the K-Means Algorithm on Data Matrix X, Where Each Row of X\n",
    "    is a Single Example.\n",
    "    \"\"\"\n",
    "    # Set m, n and K (Number of Clusters)\n",
    "    m, n = X.shape\n",
    "    K = initial_centroids.shape[0]\n",
    "    \n",
    "    centroids = initial_centroids\n",
    "    idx = np.zeros(m)\n",
    "\n",
    "    # Run K-Means\n",
    "    for i in range(max_iters):\n",
    "        \n",
    "        #Output Progress\n",
    "        print(\"K-Means Iteration %d/%d\" % (i, max_iters-1))\n",
    "        \n",
    "        # For Each Example in X, Assign it to the Closest Centroid\n",
    "        idx = find_closest_centroids(X, centroids)\n",
    "        \n",
    "        # Given the Memberships, Compute New Centroids\n",
    "        centroids = compute_centroids(X, idx, K)\n",
    "        \n",
    "    return centroids, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kMeans_init_centroids(X, K):\n",
    "    \"\"\"\n",
    "    This function initializes K centroids that are to be \n",
    "    used in K-Means on the dataset X.\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray): Data points \n",
    "        K (int):     number of centroids/clusters\n",
    "    \n",
    "    Returns:\n",
    "        centroids (ndarray): Initialized centroids\n",
    "    \"\"\"\n",
    "    \n",
    "    # Randomly Re-Order the Indices of Examples\n",
    "    randidx = np.random.permutation(X.shape[0])\n",
    "    \n",
    "    # Take the First K Examples as Centroids\n",
    "    centroids = X[randidx[:K]]\n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n# Set K and max_iters\\nK = 3\\nmax_iters = 10\\n\\n# Set Initial Centroids by Picking Random Examples from the Dataset\\ninitial_centroids = kMeans_init_centroids(X, K)\\n\\n# Run K-Means\\ncentroids, idx = run_kMeans(X, initial_centroids, max_iters)\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implementation\n",
    "''' \n",
    "# Set K and max_iters\n",
    "K = 3\n",
    "max_iters = 10\n",
    "\n",
    "# Set Initial Centroids by Picking Random Examples from the Dataset\n",
    "initial_centroids = kMeans_init_centroids(X, K)\n",
    "\n",
    "# Run K-Means\n",
    "centroids, idx = run_kMeans(X, initial_centroids, max_iters)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
